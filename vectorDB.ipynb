{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Qdrant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "poppler_path = r\"C:\\poppler\\poppler-24.02.0\\Library\\bin\"  # Replace with your actual Poppler bin path\n",
    "os.environ[\"PATH\"] += os.pathsep + poppler_path\n",
    "# Set the path to Tesseract executable\n",
    "tesseract_path = r'C:\\Tesseract-OCR'\n",
    "\n",
    "# Add Tesseract path to the PATH environment variable\n",
    "os.environ['PATH'] += os.pathsep + tesseract_path\n",
    "\n",
    "# # Set NLTK data path\n",
    "# nltk_data_path = r'C:\\Users\\Govind\\AppData\\Roaming\\nltk_data'\n",
    "# nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Govind/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Govind/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Govind/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Govind/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Set a custom directory for NLTK data\n",
    "nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Download NLTK data to the custom directory\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\medico\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 3/3 [00:57<00:00, 19.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "# Load PDF documents\n",
    "loader = DirectoryLoader('data/', glob=\"**/*.pdf\", show_progress=True, loader_cls=UnstructuredFileLoader)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB Successfully Created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming documents and embeddings are already defined\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=70)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "url = \"http://localhost:6333\"\n",
    "qdrant = Qdrant.from_documents(\n",
    "    texts, \n",
    "    embeddings, \n",
    "    url=url,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=\"vector_db\"\n",
    ")\n",
    "\n",
    "print(\"Vector DB Successfully Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Split text into chunks\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=70)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# # Create the Vector Database\n",
    "# url = \"http://localhost:6333\"\n",
    "# qdrant = Qdrant.from_documents(\n",
    "#     texts, \n",
    "#     embeddings, \n",
    "#     url=url,\n",
    "#     prefer_grpc=False,\n",
    "#     collection_name=\"vector_db\"\n",
    "# )\n",
    "\n",
    "# print(\"Vector DB Successfully Created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.2.82.tar.gz (50.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\medico\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in e:\\medico\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in e:\\medico\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\medico\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [20 lines of output]\n",
      "      \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.9.8\u001b[0m using \u001b[94mCMake 3.30.0\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
      "      \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "      2024-07-20 00:01:31,489 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "      loading initial cache file C:\\Users\\Govind\\AppData\\Local\\Temp\\tmp1i9pw1g3\\build\\CMakeInit.txt\n",
      "      -- Building for: NMake Makefiles\n",
      "      CMake Error at CMakeLists.txt:3 (project):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "      CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      \n",
      "      \u001b[91m\u001b[1m*** CMake configuration failed\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from fastapi import FastAPI, Request, Form, Response\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "import os\n",
    "import json\n",
    "import gradio as gr\n",
    "\n",
    "# Load a local BioMistral LLM model with specified settings\n",
    "local_llm = \"BioMistral-7B.Q4_K_M.gguf\"\n",
    "llm = LlamaCpp(model_path=local_llm, temperature=0.3, max_tokens=2048, top_p=1, n_ctx=2048)\n",
    "\n",
    "# Define a prompt template to guide LLM responses\n",
    "prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Chat History: {chat_history}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer. Answer must be detailed and well explained.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create sentence embeddings model for representing text as vectors\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "# Connect to a Qdrant vector database for storing and retrieving information\n",
    "url = \"http://localhost:6333\"\n",
    "client = QdrantClient(url=url, prefer_grpc=False)\n",
    "db = Qdrant(client=client, embeddings=embeddings, collection_name=\"vector_db\")\n",
    "\n",
    "# Initialize a retriever for searching within the database\n",
    "retriever = db.as_retriever(search_kwargs={\"k\":1})\n",
    "\n",
    "# Create an empty chat history list to track conversation\n",
    "chat_history = []\n",
    "\n",
    "# Create a ConversationalRetrievalChain, combining LLM and retriever for context-aware answers\n",
    "chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)\n",
    "\n",
    "# Define the actual prediction function\n",
    "def predict(message, history):\n",
    "    # Format history for compatibility with LangChain\n",
    "    history_langchain_format = []\n",
    "\n",
    "    # Guide LLM with prompt and conversation history to generate a response\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"chat_history\", 'message'])\n",
    "    response = chain({\"question\": message, \"chat_history\": chat_history})\n",
    "\n",
    "    # Update chat history with the current turn (message and answer)\n",
    "    answer = response['answer']\n",
    "    chat_history.append((message, answer))\n",
    "\n",
    "    # Format history for Gradio interface display\n",
    "    history_langchain_format = []\n",
    "    for input_question, bot_answer in history:\n",
    "        history_langchain_format.append([input_question, bot_answer])\n",
    "\n",
    "    history_langchain_format.append([message, answer])\n",
    "\n",
    "    return history_langchain_format, answer\n",
    "\n",
    "# Create a chat interface using Gradio\n",
    "gr.ChatInterface(predict).launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
